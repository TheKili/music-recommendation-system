{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import multiprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get & Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get & Format Data from content.csv\n",
    "content = pd.read_csv('../raw_data/content.csv')\n",
    "content_df = content.groupby('track_id', as_index=False).first()\n",
    "content_df = content_df.reset_index(drop=True)\n",
    "# content_df['track_name'] = content_df['track_name'].astype(str)\n",
    "# content_df['artists'] = content_df['artists'].astype(str)\n",
    "content_df = content_df[['track_id', 'track_name', 'artists']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../raw_data/playlist_data/mpd.slice.0-999.json\n",
      "../raw_data/playlist_data/mpd.slice.1000-1999.json\n",
      "../raw_data/playlist_data/mpd.slice.2000-2999.json\n"
     ]
    }
   ],
   "source": [
    "#Print Colab-Slices Locations (to check if they're correct)\n",
    "\n",
    "slice_amount = 3\n",
    "for i in range(slice_amount):\n",
    "    #json_file_paths.append(f'../raw_data/playlist_data/mpd.slice.{i*1000}-{999+ i * 1000}.json')\n",
    "    print((f'../raw_data/playlist_data/mpd.slice.{i*1000}-{999+ i * 1000}.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your JSON file\n",
    "json_file_paths = [f'../raw_data/playlist_data/mpd.slice.{i*1000}-{999+ i * 1000}.json'\n",
    "                    for i in range(slice_amount)]\n",
    "\n",
    "# Read the JSON file\n",
    "json_data = []\n",
    "for i in range(len(json_file_paths)):\n",
    "    with open(json_file_paths[i], 'r') as file:\n",
    "        json_data.append(json.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your JSON file\n",
    "json_file_paths = [f'../raw_data/playlist_data/mpd.slice.{i*1000}-{999+ i * 1000}.json'\n",
    "                    for i in range(slice_amount)]\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "pool = multiprocessing.Pool()\n",
    "json_data = pool.map(load_json_file, json_file_paths)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Track Sum per Batch\n",
    "track_sum_per_batch = 0\n",
    "batch = 0\n",
    "for i in range(len(json_data[batch]['playlists'])):\n",
    "    playlist_i = json_data[batch]['playlists'][i]['tracks']\n",
    "    track_sum_per_batch += len(playlist_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Track Sum\n",
    "track_sum = 0\n",
    "for i in range(len(json_data)):\n",
    "    json_i = json_data[i]['playlists']\n",
    "    for j in range(len(json_i)):\n",
    "        playlist = json_data[i]['playlists'][j]['tracks']\n",
    "        track_sum += len(playlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Track List Per Batch\n",
    "\n",
    "tracklist_per_batch = []\n",
    "batch = 0\n",
    "\n",
    "for i in range(len(json_data[batch]['playlists'])):\n",
    "    playlist_i = json_data[batch]['playlists'][i]['tracks']\n",
    "    for j in range(len(playlist_i)):\n",
    "        tracklist_per_batch.append(f'{i}' + ': ' + playlist_i[j]['artist_name'] + ' - ' + playlist_i[j]['track_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Track List\n",
    "\n",
    "\n",
    "tracklist = []\n",
    "for k in range (len(json_data)):\n",
    "    for i in range(len(json_data[k]['playlists'])):\n",
    "        playlist_i = json_data[k]['playlists'][i]['tracks']\n",
    "        for j in range(len(playlist_i)):\n",
    "            tracklist.append(f'{k*1000 + i}' + ' ||| ' + playlist_i[j]['artist_name']\n",
    "                             + ' ||| ' + playlist_i[j]['track_name'] + ' ||| ' + str(playlist_i[j]['track_uri']))\n",
    "            \n",
    "#Split Tracklist (to create DF)\n",
    "\n",
    "split_tracklist = [item.split(' ||| ') for item in tracklist]\n",
    "\n",
    "for i in range(len(split_tracklist)):\n",
    "    split_tracklist[i][3] = split_tracklist[i][3].replace('spotify:track:', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracklist = [f'{k*1000 + i} ||| {track[\"artist_name\"]} ||| {track[\"track_name\"]} ||| {track[\"track_uri\"].replace(\"spotify:track:\", \"\")}'\n",
    "             for k, data in enumerate(json_data)\n",
    "             for i, playlist in enumerate(data['playlists'])\n",
    "             for track in playlist['tracks']]\n",
    "\n",
    "split_tracklist = [item.split(' ||| ') for item in tracklist]\n",
    "tracklist_df = pd.DataFrame(split_tracklist, columns=['playlist_number', 'artist_name', 'track_name', 'track_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DF\n",
    "tracklist_df = pd.DataFrame(split_tracklist, columns=['playlist_number', 'artist_name', 'track_name', 'track_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge DFs on Track_ID (to get only the Tracks available in both Datasets)\n",
    "\n",
    "merged_df = pd.merge(tracklist_df, content_df, on='track_id', how='inner')\n",
    "clean_df = merged_df.drop(columns=['track_name_y', 'artists'])\n",
    "clean_unique = clean_df.groupby('track_id', as_index=False).first()\n",
    "merged_unique = clean_unique.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Create Dict per Track (to get all Playlists containing the Track)\n",
    "# #Not operational\n",
    "\n",
    "# trackdict = {}\n",
    "# for i in range(10):\n",
    "#     track = tracklist[i].split(' ||| ', tracklist[i])[1]\n",
    "#     trackdict[f'{i}'] = []\n",
    "#     for j in range(len(tracklist)):\n",
    "#         if track in tracklist[j]:\n",
    "#             trackdict[f'{i}'].append(tracklist[j].split(' ||| ')[0])\n",
    "#     print(f'done with track number {i}')\n",
    "# trackdict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Track_Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_df = clean_df.groupby('track_id', as_index=False).agg({'playlist_number': list})\n",
    "playlist_df = playlist_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all playlists containing the track\n",
    "\n",
    "# Specify the track_id you want to locate\n",
    "target_track_id = '7zkLpY72g6lKQbiHDqri1S'\n",
    "\n",
    "# Use loc[] to locate the rows with the specific track_id\n",
    "specific_rows = playlist_df.loc[playlist_df['track_id'] == target_track_id]\n",
    "\n",
    "playlistlist = specific_rows['playlist_number'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # One Hot Encode playlists into df\n",
    "\n",
    "# # Get all unique playlists\n",
    "# unique_playlists = []\n",
    "# for i in range(len(playlist_df)):\n",
    "#     unique_playlists += playlist_df['playlist_number'][i]\n",
    "# unique_playlists = list(set(unique_playlists))\n",
    "# unique_playlists.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_playlists = playlist_df['playlist_number'].explode().unique()\n",
    "unique_playlists.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create track_to_playlist_matrix with shape:\n",
    "# (len(playlist_df) = Amount of unique Tracks, len(unique_playlists) = Amount of unique Playlists)\n",
    "# Index: (playlist_df['track_id'], unique_playlists)\n",
    "\n",
    "track_to_playlist_matrix = np.zeros((playlist_df.shape[0], len(unique_playlists)))\n",
    "\n",
    "for i, playlists in enumerate(playlist_df['playlist_number']):\n",
    "    track_to_playlist_matrix[i, np.searchsorted(unique_playlists, playlists)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "track_matrix_2 = track_to_playlist_matrix @ track_to_playlist_matrix.T\n",
    "track_matrix_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create track_matrix with shape:\n",
    "# (len(playlist_df) = Amount of unique Tracks)**2\n",
    "# Index: (playlist_df['track_id'], playlist_df['track_id'])\n",
    "# with track_matrix[i][j] = amount of same playlists for track i and track j\n",
    "\n",
    "track_matrix = track_to_playlist_matrix @ track_to_playlist_matrix.T\n",
    "track_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give maximum value in matrix\n",
    "max_value = np.max(track_matrix)\n",
    "max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize = True\n",
    "if normalize == True:\n",
    "    #normalize track_matrix\n",
    "    for i in range(len(track_matrix)):\n",
    "        if track_matrix[i, i] != 1:\n",
    "            # Normalize values\n",
    "            track_matrix[i, :] /= track_matrix[i, i]\n",
    "            track_matrix[:, i] /= track_matrix[i, i]\n",
    "type(track_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_colab_recommendations (track_matrix: np.ndarray, track_id):\n",
    "    \"\"\"\n",
    "    Returns a list of recommended tracks for a given track_id\n",
    "    Based on track_matrix\n",
    "    (aka tracks that are in the same playlists as the given track_id)\n",
    "    \"\"\"\n",
    "    idx = playlist_df.loc[playlist_df['track_id'] == target_track_id].index[0]\n",
    "    track_matrix[idx]\n",
    "    sorted_indices = np.argsort(-track_matrix[idx])\n",
    "    sorted_values = track_matrix[idx][sorted_indices]\n",
    "    merged_array = np.column_stack((sorted_indices, sorted_values))\n",
    "    return merged_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_colab_recommendations(track_matrix: np.ndarray, target_track_id):\n",
    "    \"\"\"\n",
    "    Returns a list of recommended tracks for a given track_id\n",
    "    Based on track_matrix\n",
    "    (aka tracks that are in the same playlists as the given track_id)\n",
    "    \"\"\"\n",
    "    idx = np.where(playlist_df['track_id'] == target_track_id)[0][0]\n",
    "    sorted_indices = np.argsort(-track_matrix[idx])\n",
    "    merged_array = np.column_stack((sorted_indices, track_matrix[idx][sorted_indices]))\n",
    "    return merged_array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hier könnten wir die daten normalisieren, also alle werte auf 0-1 bringen   \n",
    "wenn ein track in vielen verschiedenen playlists landet   \n",
    "(aktuelles maximum: 139x) könnte eine cooccurence weniger aussagekräftig sein   \n",
    "dazu: jede cooccurence teilen   \n",
    "durch: Wert auf der Diagonalen (also wie oft ein track in der playlist vorkommt)   \n",
    "dann weighting Wert Alpha als Variable einbauen   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
